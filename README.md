# ResearchRanger - Autonomous Research Agent with RAG and Citation Tracing

A powerful research assistant that helps researchers, students, and product teams ingest documents (PDFs, web pages, notes), answer questions with source citations, and export referenceable summaries.

## Features

- ğŸ“„ **Document Ingestion**: Upload PDFs, webpages, and markdown files
- ğŸ” **Intelligent QA**: Ask questions and get answers with precise citations
- ğŸ“š **Citation Tracking**: Page/paragraph level citation provenance
- ğŸ“ **Export Capabilities**: Generate Markdown summaries and BibTeX references
- ğŸ¯ **Evidence-Based Answers**: Every answer includes source snippets and metadata
- âš¡ **Background Processing**: Asynchronous document ingestion pipeline

## Tech Stack

### Backend
- **Framework**: FastAPI
- **RAG**: LangChain
- **Workers**: Celery + Redis
- **Database**: PostgreSQL
- **Vector DB**: Chroma (local) / Pinecone (production)
- **Storage**: S3-compatible (local filesystem for dev)
- **PDF Parsing**: PyMuPDF

### Frontend
- **Framework**: Next.js 14 (App Router)
- **UI**: React with streaming chat interface
- **Styling**: Tailwind CSS

## Project Structure

```
.
â”œâ”€â”€ backend/          # FastAPI application
â”œâ”€â”€ frontend/         # Next.js application
â”œâ”€â”€ workers/          # Celery workers
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md
```

## Quick Start

### Prerequisites
- Python 3.11+
- Node.js 18+
- Docker & Docker Compose
- PostgreSQL
- Redis

### Local Development

1. **Clone and setup backend**:
```bash
cd backend
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```

2. **Setup frontend**:
```bash
cd frontend
npm install
```

3. **Start services** (using Docker Compose):
```bash
docker-compose up -d
```

4. **Run migrations**:
```bash
cd backend
alembic upgrade head
```

5. **Start backend**:
```bash
cd backend
uvicorn app.main:app --reload
```

6. **Start frontend**:
```bash
cd frontend
npm run dev
```

## Architecture

### Ingestion Flow
1. User uploads document â†’ API stores file
2. Celery worker extracts text (PDF/HTML/MD)
3. Document is chunked with overlap and metadata
4. Embeddings generated and stored in vector DB
5. Metadata (file, page, paragraph, offsets) linked to chunks

### QA Flow
1. User asks question â†’ Query embedded
2. Top-K similar chunks retrieved from vector DB
3. Evidence-aware prompt constructed with source snippets
4. LLM generates answer with inline citations
5. Citations parsed and linked back to source documents
6. Response streamed to frontend with source highlights

## License

MIT

